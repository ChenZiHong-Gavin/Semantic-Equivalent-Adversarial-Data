[toc]

# 0 开始之前

花了大把时间在另一篇论文上面

# 1 背景与介绍

## 1.1 视觉问答

[VQA_Demo](../util/0. VQA_Demo)

### 1.1.1 视觉问答是什么

视觉问答Visual Question Answer (VQA) 是对视觉图像的自然语言问答，作为视觉理解 (Visual Understanding) 的一个研究方向，连接着视觉和语言，模型需要在理解图像的基础上，根据具体的问题然后做出回答。

下面是一个视觉问答的小例子（源自知乎）：

想象一个系统，它可以回答这些问题：

- 图像中有什么？
- 有人类吗?
- 什么运动正在进行？
- 谁在踢球?
- 图像中有多少球员？
- 参赛者有哪些人?
- 在下雨吗?

![img](https://pic4.zhimg.com/80/v2-bcae40fa7dc4a586efeec0ad5504736f_720w.jpg)

作为人类，我们可以很轻松执行这项任务，但是研究具有这种功能的系统似乎有些困难。

然而，随着深度学习（DL）的出现，我们目睹了视觉问答（VQA）方面的巨大研究进展，使得能够回答这些问题的系统正在出现，并带来很有希望的成果。

### 1.1.2 视觉问答的简单原理

当被问到：

**巴黎有几座桥梁？**

视觉问答中的**NLP问答系统**通常会：

- 分类或输入问题：这是一个“多少”问题，因此答复必须是一个数字。
- 提取对象以计数：桥梁。
- 提取必须执行计数的上下文：在这种情况下，巴黎市。

这是基于普通的NLP问答系统而言的，对于VQA来说：

- 搜索和推理部分必须在图像内容中进行

过程如下：

1. 先对图像image和问题question提取特征

2. 联合这些特征做一些多模态融合（如element-wise product, MCB，MFB），attention，知识补充等处理

   简单的多模态的特征融合方法有**element-wise product (multiply) / element-wise sum,  concatenation**，增加额外层的 **concatenation + FCL/CNN/LSTM** 等等

   ![img](https://upload-images.jianshu.io/upload_images/11731515-ca5ba41a1be776d1.png?imageMogr2/auto-orient/strip|imageView2/2/w/743/format/webp)

3. 经过分类器输出answer

对于图像image：使用VGG，Resnet，……

对于问题question：使用LSTM, GRU, ……

### 1.1.3 准确率

![img](https://upload-images.jianshu.io/upload_images/11731515-4756eaf57425b688.png?imageMogr2/auto-orient/strip|imageView2/2/w/722/format/webp)

下表列出一些模型的准确率（该图是在介绍BUTD的论文中出现的）：

![img](https://upload-images.jianshu.io/upload_images/11731515-572fabf2c7c4f1b5.png?imageMogr2/auto-orient/strip|imageView2/2/w/725/format/webp)

### 1.1.4 总结

作为需要视觉理解与推理能力的，介于Vision与NLP间的视觉问答VQA，是一个有趣而又充满挑战的问题。它的进步不仅依赖于计算机视觉的发展和自然语言处理的能力，还需要对图像的理解——视觉基础能力，如识别，检测等，同时学习到知识与推理的能力。然而，这条路还有很长的距离要走。

在这里我们得到一个关键的点：现有的算法总是可以从更多的训练数据中受益匪浅。

因此，自动化数据扩充是一项很关键的技术。数据对模型来说是燃料。

> 参考文献：
>
> *VQA: Visual Question Answering*
>
> *Introduction to Visual Question Answering: Datasets, Approaches and Evaluation*
>
> *Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering*

## 1.2 视觉问答的数据增强

### 1.2.1 数据增强是什么？

数据增强也叫数据扩增，意思是在**不实质性的增加数据**的情况下，让**有限的数据产生等价于更多数据的价值**。

数据扩增的目的就是使得训练数据尽可能的接近测试数据，从而提高预测精度。另外数据扩增可以迫使网络学习到更鲁棒性的特征，从而使模型拥有更强的泛化能力。

### 1.2.2 常规图像数据增强方式

一般的数据增强方式有两种：

- 图像内容变换（data warping）

  包括几何、颜色变换，随机擦除，对抗训练，风格

  ![深度学习中的数据增强方法都有哪些？](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9zdGF0aWMwMDEuaW5mb3EuY24vcmVzb3VyY2UvaW1hZ2UvNTkvMTYvNTk4OGMyNmE0YzcwODYzZjdmNTFkOGM2YTAyMzEwMTYuanBn?x-oss-process=image/format,png)

- 迁移图像采样（oversampling）

  训练的时候对少的一类图像进行重复选择

### 1.2.3 VQA增强工作的问题

对于VQA的数据增强目前也没有相关工作，现有的基于图像的增强方案（如旋转和翻转）都不能直接应用于VQA，这是因为增强的同时还需要维持语义的正确性。

一个与方向相关的Question-Answer对，如果相关的图像被旋转或翻转，对于问题的回答可能不是真的。

例如，当问到电脑的位置是什么时，汽车是在垃圾桶的左边还是右边？使用一般方法进行数据增强之后结果可能是相反的答案。随机抹去与问题相关的图像，就会弄错物体的数量。

这样的转换是不可用的。

以前的工作基于图像内容和给定答案生成合理的问题，即视觉问题生成（VQG）。

VQG，Visual Question Generation，给定一张图片，来生成一个流畅且切合上下文主题的问句。

![img](https://pic3.zhimg.com/80/v2-1792b252cfb003d92c1e7728b1bc11aa_720w.jpg)

然而，相当一部分生成的问题要么有语法错误，要么措辞怪异。

此外，他们从同一目标数据集中的问题和图像中学习，因此生成的数据与原始数据的分布相同。

由于训练数据和测试数据通常不具有相同的分布，生成的数据不能帮助缓解过度拟合的问题。

### 1.2.4 VQA数据增强

本文中，我们没有直接操作图像和问题，而是使用生成的图像和问题的对抗性例子作为作为增强的数据。

增强后的例子不会改变图像和问题中的视觉属性以及问题的语义。画面、问题、答案的正确性仍然保持。

### 1.2.5 总结

为了使VQA的模型更加细化精准，我们需要基于现有的画面、问题、答案进行数据扩增。

但是无论是单独对图像进行变换还是对问题与答案进行变换都不能达到我们想要的要求。

因为，在本篇论文中，我们决定使用图像和问题的对抗性例子作为作为增强的数据。

关于对抗性例子的生成，将在[下一节](# 对抗性测试生成)中讲到。

> 参考文献：
>
> *Generating diverse questions using variational autoencoders*
>
> *Visual question generation as dual task of visual question answering*

## 1.3 对抗性攻击

> adverserial attack

[FGSM](../util/LOG.md)

### 1.3.1 什么是对抗攻击

对输入样本故意添加一些人无法察觉的细微的干扰，导致模型以高置信度给出一个错误的输出。

以下面的图片作为直观解释：

对抗攻击就是使得DNN误判的同时，使得图片的改变尽可能少。

### 1.3.2 对抗攻击方法

有以下方法：

1. 像素修改

   可以针对一张已经有正确分类的image，对其进行细微的像素修改，可以在DNN下被错分为其他label。(图片来自知乎)

   ![img](https://pic4.zhimg.com/80/v2-ed60089ae25c81ba2677ec34ffa2a47f_720w.jpg)

   样本x的label为熊猫，在对x添加部分干扰后，在人眼中仍然分为熊猫，但对深度模型，却将其错分为长臂猿，且给出了高达99.3%的置信度

2. 像素攻击

   改动图片上的一个像素，就能让神经网络认错图，甚至还可以诱导它返回特定的结果。(图片来自知乎)

   ![img](https://pic2.zhimg.com/80/v2-59a3afcc069df94927ffe1efd62822e9_720w.jpg)

   改动图片上的一个像素，就能让神经网络认错图，甚至还可以诱导它返回特定的结果

   比如说上图中把船认成车。

3. 无意义的image

   同样，根据DNN，很容易产生一张在人眼下毫无意义的image，但是在DNN中能够获得高confidence的label（图片来自知乎）

   ![img](https://pic1.zhimg.com/80/v2-0390bba1f2c35220c8b099b8ab0f4ebc_720w.jpg)

   两种EA算法生成的样本，这些样本人类完全无法识别，但深度学习模型会以高置信度对它们进行分类，例如将噪声识别为狮子。



### 1.3.3 对抗性攻击的作用

DNN在很多方面已经展示出比人类要好的水平,比如image classification,machine translation等等。

DNN的可攻击性，导致了DNN在一些重要领域之内无法大规模部署，极大的限制了DNN的发展。

对对抗攻击有了比较深入的理解之后,才能对对抗防御有比较深入的理解。

对于VQA而言，对抗攻击形成的数据扩增可以对模型进行优化。

### 1.3.4 总结

对抗性对输入样本故意添加一些人无法察觉的细微的干扰，导致模型以高置信度给出一个错误的输出。

使用对抗性攻击可以对深度学习模型进行优化，从而提升VQA能力。

在这次实验中我们采用对抗性方式进行数据扩增。

> 参考文献：
>
> *Adversarial example generation with syntactically controlled paraphrase networks*
>
> *Semantically equivalent adversarial rules for debugging nlp models*
>
> *Explaining and harnessing adversarial examples*

## 1.4 对抗性训练

对抗训练（adversarial training）是增强神经网络鲁棒性的重要方式。在对抗训练的过程中，样本会被混合一些微小的扰动（改变很小，但是很可能造成误分类），然后使神经网络适应这种改变，从而对对抗样本具有鲁棒性。

#  2 工具实现原理与方法

### 2.1 原理-VQA

回答关于图像的问题可以被表述为这样一个问题：

根据参数化的概率指数，在给出图像v和问题q的情况下预测答案。

$$
\hat{a}=\arg \max _{a \in \mathcal{A}} p(a \mid v, q ; \theta)
$$
其中θ代表所有要学习的参数的向量，A是所有答案的集合。

我们设$$
v=\left\{\overrightarrow{v_{1}}, \overrightarrow{v_{2}}, \ldots, \overrightarrow{v_{K}}\right\}
$$是一个从K个图像区域提取的视觉特征的集合，问题是
一系列的词的集合$q=\{q 1, q 2, \ldots, q n\}$

### 2.2 原理-数据扩增

根据之前的介绍，我们产生对抗性例子作为额外的训练数据。

分以下几步：

#### 2.2.1 生成图片对抗性例子

我们采用了一种高效的基于梯度的攻击手段——迭代快速梯度法（IFGSM）来生成视觉对抗性的例子。

关于FGSM以及其拓展IFGSM的说明可以查看[FGSM,IFGSM](../doc/FGSM/LOG.md)

我们将FGSM应用于视觉输入：
$$
v_{a d v}=v+\epsilon \operatorname{sign}\left(\nabla_{v} L\left(\theta, v, q, a_{t r u e}\right)\right)
$$

其中，$v_{adv}$是v的对抗性例子，θ是模型参数的集合，$
L\left(\theta, v, q, a_{\text {lrue }}\right)
$表示用于训练VQA模型的成本函数，$\epsilon$为敌意扰动的大小。

攻击将梯度反向传播到输入的视觉特征，计算$\nabla_{v} L\left(\theta, v, q, a_{\text {true }}\right)$同时固定网络的参数。

然后，它通过一个小的方向调整输入（即$\nabla_{v} L\left(\theta, v, q, a_{\text {true }}\right)$），使损失最大化。

由此产生的扰动，$v_{adv}$，被VQA模型错误分类。

FGSM的一个直接扩展是以较小的步长多次应用它，称为IFGSM。
$$
v_{a d v}^{0}=v, \quad v_{a d v}^{N+1}=\operatorname{Clip}_{v, \epsilon}\left\{v_{a d v}^{N}+\alpha \operatorname{sign}\left(\nabla_{v} L\left(\theta, v_{a d v}^{N}, q, a_{t r u e}\right)\right)\right\}
$$
其中，$$
\text { Clip }_{v, \epsilon}(A)
$$表示对A的元素进行剪裁，A_{i,j}剪裁的范围是_$$\left[v_{i, j}-\epsilon, v_{i, j}+\epsilon\right]$$，a是每次迭代的步长。

我们将基于梯度的方法总结为基于梯度的方法称为VAdvGen(v, q)。

#### 2.2.2 生成语义等价的问题

我们使用一个纯粹基于神经网络的转述模型来生成语义等价的问题。

它是NMT框架的一个拓展。

关于NMT框架，可见文档[NMT](../doc/nmt)

在这个神经编码器-解码器框架中，编码器(RNN)被用来将源句的含义压缩成一串的向量，解码器是一个有条件的RNN语言模型，它逐字生成目标句子。

编码器采用一连串的原始问题词$X=\left\{x_{1}, \ldots, x_{T_{x}}\right\}$作为输入，并产生一连串的上下文。

在给定源句子的条件下，解码器产生目标句子的概率分布$Y=\left\{y_{1}, \ldots, y_{T_{y}}\right\}$

原理如下：
$$
P(Y \mid X)=\prod_{t=1}^{T_{y}} P\left(y_{t} \mid y_{<t}, X\right)
$$
举个例子，我们把英语句子E1翻译成一个法语句子F。

接下来，F被翻译回英语，从而得到一个英语句子的概率分布E2
$$
P\left(E_{2} \mid E_{1}, F\right)=P\left(E_{2} \mid F\right)
$$
我们的转义模型选取前K个表现最好的译文集合$F=\left\{F_{1}, \ldots, F_{k}\right\}$进行转换，确保原来句子的语义和句法都能被捕获。

将多个分句翻译成一个句子，这个句子产生了针对目标词汇的概率分布：
$$
P\left(y_{t}=w \mid y_{<t}, \mathcal{F}\right)=\sum_{i=1}^{K} P\left(\mathcal{F}_{i} \mid E_{1}\right) \cdot P\left(y_{t}=w \mid y_{<t}, \mathcal{F}_{i}\right)
$$
把原来的句子分别转换成法语和葡萄牙语，得到两个分布
$$
P\left(y_{t}=w \mid y_{<t}, \mathcal{F}^{F r}\right) \text { 和 } P\left(y_{t}=w \mid y_{<t}, \mathcal{F}^{\dot{P} o}\right) \text {. }
$$
然后平均一下这两个分布
$$
P\left(y_{t}=w \mid y_{<t}, \mathcal{F}^{F r}, \mathcal{F}^{P o}\right)=\frac{1}{2}\left(P\left(y_{t}=w \mid y_{<t}, \mathcal{F}^{F r}\right)+P\left(y_{t}=w \mid y_{<t}, \mathcal{F}^{P o}\right)\right)
$$
产生一个多语句，多语义的转义概率。

最后得到句子的概率分布：
$$
P(E 2 \mid E 1)=\prod_{t=1}^{T_{E_{2}}} P\left(y_{t} \mid y_{<t},{\mathcal{F}}^{F r},{\mathcal{F}}^{P o}\right)
$$
由此，我们生成了语义等价的问题。

#### 2.2.3 用增强的用例进行对抗性训练

我们可以得到两个视觉对抗用例：

- 用原有图片和问题进行IFGSM图像扩增
  $$
  v_{q c}=\operatorname{VAdvGen}(v, q)
  $$

- 

- 用原有图片和转义的问题进行IFGSM图像扩增
  $$
  \quad v_{q a d v}=\operatorname{VAdvGen}\left(v, q_{a d v}\right)
  $$

由此可以产生四个额外的训练对，分别是：

- （$v_{qc}$，q）
- （$v_{qadv}$，q）
- （$v_{qc}$，$q_{adv}$）
- （$v_{adv}$，$q_{adv}$）

它们是语义相同的，所以它们的回答是相同的。

### 2.3 核心算法-对抗性训练方案

以下是对抗性训练方案的伪代码：

**Input: 一组纯净数据图像 v, 问题 q 和答案 a** 

**Output: 神经网络参数 θ**

1：$q_{a d v}=\mathrm{QAdvGen}(q)$  生成转义问题

2：$for\ each\ training\ step\ i\ do$

3：	采样一小批干净的图像$v^b$和文本$q^b$以及文本对抗性用例$q^b_{adv}$以及它们的答案$a^b$

4：	$if\ i$ 是在对抗训练的阶段

5：		$then$ 生成对应的对抗性测试训练对（$v_{qc}$，q），（$v_{qadv}$，q），（$v_{qc}$，$q_{adv}$），（$v_{adv}$，$q_{adv}$）

6：			最小化损失函数Loss()（这个函数将在下面的评估内容中提及）

7：	$else$

8：		最小化损失函数$L\left(\theta, v, q, a_{\text {true }}\right)$

9 ：return  θ

### 2.4 突出创新点

#  未完待续

# 3 工具实现过程

## 3.1 确定数据集

### 3.1.1 论文中使用的数据集

论文在在VQAv2上进行了实验。

VQAv2包含443K训练，214K验证和453K测试。

现在有很多关于VQA的数据集，比如说Visual Madlibs、TDIUC等等。

但是它们都太大了，动辄几十个G。（让没有电脑配置的我瑟瑟发抖，果然很多深度学习研究是要靠数据喂出来的）

### 3.1.2 我使用的数据集

因此我选择了DAQUAR（DAtaset for QUestion Answering on Real-world images），它是最早也是最小的数据集，包含了6795张训练数据和5673张测试数据，我自己做了一点精简处理。

图像数据可以从[官网]([Max-Planck-Institut für Informatik: Visual Turing Challenge (mpg.de)](https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/research/vision-and-language/visual-turing-challenge/#c10216))下载。

训练和测试使用的Q-A对放在了`data/raw_data`目录中。

### 3.1.3 数据集预处理

当加载我们的csv文件时，我们可以看到我们的问题和答案是以一种奇怪的方式排列。

最好的办法是在一栏中写上问题，另一栏中写上它所指的图片，第三栏中写上答案。

``` python
import os 
import csv

def prepare_data(in_directory,out_directory, mode):
    # 找到原来的数据
    file_name_in=os.path.join(in_directory,'DAQUAR_{}_raw.csv'.format(str(mode)))
    file_name_out=os.path.join(out_directory,'DAQUAR_{}_processed.csv'.format(str(mode)))
    
    # 读数据
    with open(file_name_in, 'r') as f, open(file_name_out, 'w', newline='') as f_out:
        reader = csv.reader(f)
        
        fieldnames=['question','image','answer']
        writer = csv.DictWriter(f_out, fieldnames=fieldnames)
        
        writer.writeheader()
        
        row_skip=2
        dico={'question':None,
              'image':None,
              'answer':None}

        for index, row in enumerate(reader):
            
            # 偶数就是问题
            if index % row_skip ==0:
                # 把问题和图像分开
                question_image_list=row[0].split('image')

                dico['question']=[question_image_list[0]]
                
                dico['image']='image'+question_image_list[1].replace(' ?','')
            
            else:
                dico['answer']=row
                
                # 写入csv
                writer.writerow({'question': dico['question'], 'image':dico['image'], 'answer': dico['answer']})

                dico={'question':None,
                 'image':None,
                'answer':None}
```

处理后的数据如下：

|      |                                                 0 |      1 |                                      2 |
| ---: | ------------------------------------------------: | -----: | -------------------------------------: |
|    0 |                                          question |  image |                                 answer |
|    1 |  ['﻿what is on the right side of the black tele... | image3 |                               ['desk'] |
|    2 | ['what is in front of the white door on the le... | image3 |                          ['telephone'] |
|    3 |                   ['what is on the desk in the '] | image3 | ['book scissor papers tape_dispenser'] |
|    4 |    ['what is the largest brown objects in this '] | image3 |                             ['carton'] |

放在`data/processed_data`目录

## 3.2 实现VQA架构

详细内容在文档[LOG](../util/0. VQA_Demo/LOG.md)中有说明，这里只截取关键部分。

### 3.2.1 论文中使用的VQA架构

论文中使用的是Bottom-Up-Attention and TopDown (BUTD) ，它结合了自下而上和自上而下的

注意力机制来执行VQA。自下而上的机制从Faster R-CNN中生成对象建议，自上而下的机制预测

这些建议的注意力分布。

该模型在2017年VQA挑战赛中获得第一名。

![rcnn_example](../../seada-vqa/fig/rcnn_example.png)

> 参考文献：
>
> *Bottom-up and top-down attention for image captioning and visual question answering*
>
> *Proceedings of the IEEE international conference on computer vision*

但是由于BUTD的运行要求，我决定放弃使用这个VQA框架。

BUTD要求：

任何拥有12GB或更大内存的NVIDIA GPU，用于训练Faster R-CNN ResNet-101，200GB或更大的存储空间用于存放训练数据

### 3.2.2 我使用的VQA架构

BUTD的硬件要求使得我在没有实验室设备支持的情况下无法顺利进行这项实验。

考虑到这次复现起到的绝大部分是演示作用，我决定采用一个简单的VQA demo，该demo由VGG Net (VGG-16)和斯坦福大学的Word2Vec(Glove)组成，采用一个简单的模型，合了图像和词嵌入的特征并运行一个多层感知器。

这个工具是用来演示VQA的，因此重点在于简单而不是速度。为了方便演示，采用的是web的jupyter book形式。

####  运行要求-依赖

1. Keras 2.0以上版本

   * 基于python的模块化深度学习库

2. Tensorflow 1.2+

3. scikit-learn

   * python的基本机器库

4. Spacy version 2.0+

   *  用来加载Glove向量（word2vec）。
   *  要升级和安装Glove向量
   *  python -m spacy download en_vectors_web_lg
   *  * python -m spacy download en_vectors_web_lg

5. OpenCV 

   * OpenCV只用于调整图像的大小和改变颜色通道。
   * 你可以使用其他库，只要你能传递一个224x224的BGR图像（注意：BGR而不是RGB）。

6. VGG 16预训练权重

   * 请下载权重文件 [vgg16_weights.h5](https://drive.google.com/file/d/1xJbtMZzKv62PaohN1fRySZR6l9gHTz6Z/view?usp=sharing)

另外：建议使用anaconda配置环境，不然一个包一个包的寻找冲突与依赖会非常麻烦(亲历)。

部分安装指令：

- 安装cv2

  `conda install opencv`

- 安装matplotlib

  `conda install matplotlib`

- 安装sklearn(conda下的名字不太一样)

  `conda install -c anaconda scikit-learn`

#### 模型原理

这使用了一个经典的CNN-LSTM模型，如下图所示，图像特征和语言特征被分别计算并结合在一起，使用一个多层感知器被用于训练综合特征。

![model](../../seada-vqa/fig/model.png)

- 获取图像特征

  - 预训练 VGG Net (VGG-16)

    > 虽然VGG Net并不是图像特征最好的CNN模型，GoogLeNet（2014年冠军）和ResNet（2015年冠军）的分类分数更胜一筹，但VGG Net的功能非常多，简单，相对较小，更重要的是使用起来方便。
    >
    > 作为参考，这里是VGG 16在ILSVRC-2012上的表现 
    >
    > ![rate](../../seada-vqa/fig/rate.png)

    我们直接使用了预训练模型。

  - 读取模型

    ``` python
    def get_image_model(CNN_weights_file_name):
        # 接收CNN权重文件
        # 返回带有该权重的VGG模型更新
        # 需要使用models/CNN中的VGG.py文件
        from models.CNN.VGG import VGG_16
        image_model = VGG_16(CNN_weights_file_name)
        image_model.layers.pop()
        image_model.layers.pop()
        # 不包含最后两层的标准VGG文件
        sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)
        image_model.compile(optimizer=sgd, loss='categorical_crossentropy')
        return image_model
    ```

  - 提取图像特征

    原先的特征提取逻辑是：

    取原始图像，通过模型网络一直运行，直到到达最后一层。

    我们不打算使用VGG的最后两层，因为VGG Net的最后一层是1000路softmax，而最后第二层是Dropout。

    从VGG-16中提取4096维的图像特征的过程：

    ``` python
    def get_image_features(image_file_name):
        # 使用给定的图像文件运行VGG 16模型
        # 将权重作为(1,4096)维向量返回
        image_features = np.zeros((1, 4096))
    
        # 由于VGG被训练成224x224的图像，每一个新图形都需要经过同样的转换
        im = cv2.resize(cv2.imread(image_file_name), (224, 224))
        # 把图像转为RGBA格式
        im = im.transpose((2,0,1)) 
    
        
        # 这个轴的维度是必须的，因为VGG是在1, 3, 224, 224维度上训练的
        # 尽管我们只使用一张图片，但我们必须保持维度的一致
        im = np.expand_dims(im, axis=0) 
    
        image_features[0,:] = model_vgg.predict(im)[0]
        return image_features
    ```

- 获取文字特征

  我们将使用斯坦福大学的Word2Vec，称为[Glove](http://nlp.stanford.edu/projects/glove/)。Glove将一个给定的标记简化为300维的表示。

  ``` python
  def get_question_features(question):
      # 对于一个给定的问题，一个unicode字符串，返回使用Glove Vector计算的时间序列向量
      word_embeddings = spacy.load('en_vectors_web_lg')
      tokens = word_embeddings(question)
      question_tensor = np.zeros((1, 30, 300))
      for j in range(len(tokens)):
          question_tensor[0,j,:] = tokens[j].vector
      return question_tensor
  ```

- 结合了图像和词嵌入的特征，运行一个多层感知器，实现VQA

  ``` python
  def get_VQA_model(VQA_model_file_name, VQA_weights_file_name):
      # 给定VQA模型和它的权重，编译并返回更新后的模型
      vqa_model = model_from_json(open(VQA_model_file_name).read())
      vqa_model.load_weights(VQA_weights_file_name)
      vqa_model.compile(loss='categorical_crossentropy', optimizer='rmsprop')
      return vqa_model
  ```

至此，我们完成了一个简单的VQA模型。

实际场景的演示可以观看视频。

#### VQA数据交互

输入图片路径和问题：

``` python
image_file_name = 'test.jpg'
question = u"What vehicle is in the picture?"
```

如下：

**What vehicle is in the picture ?**

![test](../util/0.%20VQA_Demo/test.jpg)

调用上文中提到的获取特征的方法

``` python
# 获得图像特征
image_features = get_image_features(image_file_name)
```

``` python
# 获取问题特征
question_features = get_question_features(question)
```

进行训练和预测：

``` python
y_output = model_vqa.predict([question_features, image_features])
warnings.filterwarnings("ignore", category=DeprecationWarning)
labelencoder = joblib.load(label_encoder_file_name)
for label in reversed(np.argsort(y_output)[0,-5:]):
    print(str(round(y_output[0,label]*100,2)).zfill(5), "% ", labelencoder.inverse_transform(label.reshape(-1,1)))
```

结果由上面的方法打印出来：

**51.87 % train
031.5 % bicycle
03.81 % bike
02.91 % bus
02.54 % scooter**

由此，VQA模型的功能展示完毕。

> 注意：
>
> 完整的代码可以在[code](../util/0. VQA_Demo/Visual_Question_Answering_Demo_in_python_notebook.ipynb)找到。
>
> 更加详细的文档说明以及更多的VQA例子请见[LOG](../util/0. VQA_Demo/LOG.md)。

## 3.3 实现FGSM和IFGSM

详细内容在文档[LOG](../util/1. FGSM_Demo/LOG.md)中有说明，这里只截取关键部分。

### FGSM

#### 运行要求-依赖

1. Keras

   TensorFlow backend

2. numpy

3. matplotlib

   演示绘图使用

4. 模型

   https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels.h5

#### 模型原理

正如我们上文所说的，对抗攻击是对输入样本故意添加一些人无法察觉的细微的干扰，导致模型以高置信度给出一个错误的输出。

FGSM是Goodfellow中描述的最简单的生成对抗图像的方法之一。

通过线性化损失函数并求解扰动来最大化受$L_{\infty}$约束的扰动，可以以解析解的方式完成此操作，但需要进行一次反向传播的调用：
$$
X^{a d v}=X+\epsilon \operatorname{sign}\left(\nabla_{x} J\left(X, y_{\text {true }}\right)\right)
$$
IFGSM

我们引入了一种简单的方法来扩展“快速”方法，我们以较小的步长将其多次应用，并在每一步之后剪切中间结果的像素值，以确保它们位于原始图像的值域中：
$$
X_{0}^{a d v}=X, \quad X_{N+1}^{a d v}=C l i p_{X, \epsilon}\left\{X_{N}^{a d v}+\alpha \operatorname{sign}\left(\nabla_{x} J\left(X_{N}^{a d v}, y_{\text {true }}\right)\right)\right\}X_{0}^{a d v}=X, \quad X_{N+1}^{a d v}=C l i p_{X, \epsilon}\left\{X_{N}^{a d v}+\alpha \operatorname{sign}\left(\nabla_{x} J\left(X_{N}^{a d v}, y_{\text {true }}\right)\right)\right\}
$$


- 有目标的对抗攻击

``` python
for i in range(epochs): 
    target = K.one_hot(target_class, 1000)
    
    # 获得损失和相对于输入的损失梯度
    loss = -1*K.categorical_crossentropy(target, model.output)
    grads = K.gradients(loss, model.input)

    # 获取梯度符号
    delta = K.sign(grads[0])
    x_noise = x_noise + delta

    # 扰乱图像
    x_adv = x_adv + epsilon*delta

    # 获取新的图像进行预测
    x_adv = sess.run(x_adv, feed_dict={model.input:x})
    preds = model.predict(x_adv)
```

- 无目标的对抗攻击

``` python
for i in range(epochs): 
    target = K.one_hot(initial_class, 1000)
    
    loss = K.categorical_crossentropy(target, model.output)
    grads = K.gradients(loss, model.input)

    delta = K.sign(grads[0])
    x_noise = x_noise + delta

    x_adv = x_adv + epsilon*delta

    x_adv = sess.run(x_adv, feed_dict={model.input:x})
    preds = model.predict(x_adv)

    prev_probs.append(preds[0][initial_class])

    if i%20==0:
        print(i, preds[0][initial_class], vgg16.decode_predictions(preds, top=3)[0])
```

#### IFGSM数据交互

**对于有目标的对抗攻击：**

输入初始图片：

``` python
img_path = 'orange.jpg'
img = image.load_img(img_path, target_size=(224,224))
```

模型进行预测，得到一个比较准确的结果：

**Predicted: [('n07747607', 'orange', 0.9942152), ('n07749582', 'lemon', 0.0041732006), ('n03991062', 'pot', 0.00032574133)]**

进行梯度攻击后，获得新的图像并进行预测，得到的结果如下：

**0 4.3977587e-05 [('n07747607', 'orange', 0.99379504), ('n07749582', 'lemon', 0.00440125), ('n03991062', 'pot', 0.00035894263)]
20 0.0008221095 [('n07747607', 'orange', 0.9743702), ('n07749582', 'lemon', 0.01145205), ('n03991062', 'pot', 0.0020397978)]
40 0.008314591 [('n07747607', 'orange', 0.90192163), ('n07749582', 'lemon', 0.024204142), ('n07718472', 'cucumber', 0.008314591)]
60 0.04159412 [('n07747607', 'orange', 0.7212007), ('n07718472', 'cucumber', 0.04159412), ('n07749582', 'lemon', 0.03562256)]
80 0.104958005 [('n07747607', 'orange', 0.46788505), ('n07718472', 'cucumber', 0.104958005), ('n07749582', 'lemon', 0.037138414)]**

会发现，经过几次迭代之后，预测成功的准确率迅速下降。

而预测成为目标类别，也就是`cucumber`迅速上升：

![res_orange](../util/1.%20FGSM_Demo/res_orange.png)



**对于无目标的对抗攻击**

输入初始图片：

``` python
# 导入火烈鸟图片
img_path = 'flamingo.jpg'
img = image.load_img(img_path, target_size=(224,224))
```

模型进行预测，得到一个比较准确的结果：

![flamingo](../util/1.%20FGSM_Demo/flamingo.jpg)

**Predicted: [('n02007558', 'flamingo', 0.99999607), ('n02006656', 'spoonbill', 1.8544616e-06), ('n02009912', 'American_egret', 8.5071696e-07)]**

进行梯度攻击后，获得新的图像并进行预测，得到的结果如下：

**0 0.9999956 [('n02007558', 'flamingo', 0.9999956), ('n02006656', 'spoonbill', 2.0940493e-06), ('n02009912', 'American_egret', 9.710699e-07)]**
**20 0.9999491 [('n02007558', 'flamingo', 0.9999491), ('n02006656', 'spoonbill', 1.9610497e-05), ('n02009912', 'American_egret', 1.08507675e-05)]**
**40 0.999637 [('n02007558', 'flamingo', 0.999637), ('n02006656', 'spoonbill', 0.00011842114), ('n02009912', 'American_egret', 7.331572e-05)]**
**60 0.99812275 [('n02007558', 'flamingo', 0.99812275), ('n02006656', 'spoonbill', 0.0005140069), ('n02009912', 'American_egret', 0.00035199246)]**
**80 0.99239904 [('n02007558', 'flamingo', 0.99239904), ('n02006656', 'spoonbill', 0.0017014268), ('n02009912', 'American_egret', 0.0012523659)]**
**100 0.976348 [('n02007558', 'flamingo', 0.976348), ('n02006656', 'spoonbill', 0.004375022), ('n02012849', 'crane', 0.0034781264)]**
**120 0.93764174 [('n02007558', 'flamingo', 0.93764174), ('n02006656', 'spoonbill', 0.009315101), ('n02012849', 'crane', 0.00835958)]**
**140 0.85389256 [('n02007558', 'flamingo', 0.85389256), ('n02012849', 'crane', 0.017473213), ('n02006656', 'spoonbill', 0.017150726)]**

发现预测是火烈鸟的准确率迅速下降。

![crane](../util/1.%20FGSM_Demo/crane.png)

综合考虑，我们决定使用无目标的对抗攻击继续进行我们的实验。

> 注意：
>
> 完整的代码可以在[code](../util/1. FGSM_Demo/)找到。
>
> 更加详细的文档说明以及更多的VQA例子请见[LOG](../util/1. FGSM_Demo/LOG.md)。
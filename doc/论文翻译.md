# 用于视觉问答的语义等效对抗性数据扩展

> 视觉问答（Visual Question Answering,VQA）是一项结合计算机视觉和自然语言处理的学习任务。
>
> 计算机视觉主要是对给定图像进行处理，包括图像识别，图像分类等任务。
>
> 自然语言处理主要是对自然语言， 文本形式的内容进行处理以及理解，包括机器翻译，信息检索，生成文本摘要等任务。
>
> 视觉问答是需要对给定图像和问题进行处理，经过一定的视觉问答技术处理过后生成自然语言答案，是对二者的结合。
>
> 目前VQA比较先进方法包括（1）基于融合的方法：MUTAN和BLOCK；（2）基于注意力的方法：DFAF和MLIN；（3）视觉推理方法：Counting、Dual-MFA和Graph；（4）其他VQA方法：MuRel 等。

## 摘要

由于深度神经网络（DNN）的快速发展，视觉问答（VQA）已经取得了巨大成功。

另一方面，数据增强作为DNN的主要技巧之一，已被广泛用于许多计算机视觉任务。

然而，研究VQA的数据增强问题的工作很少。现有的基于图像的增强方案（如旋转和翻转）都不能直接应用于VQA，因为它的语义结构--需要正确地维护<image、question、answerer>三要素。例如，一个与方向相关的问题-答案（QA）对，如果相关的图像被旋转或翻转，这对问题可能不是真的。

本文中，我们没有直接操作图像和问题，而是使用生成的图像和问题的对抗性例子作为作为增强的数据。

增强后的例子不会改变图像和问题中的视觉属性以及问题的语义。
画面、问题、答案的正确性仍然保持。

然后，我们使用对抗性学习来训练一个经典的VQA模型（BUTD）。

我们发现，我们不仅提高了VQAv2的整体性能，而且与基线模型相比，还能有效地抵御对抗性攻击。
源代码可在https://github.com/zaynmi/seada-vqa找到

## 关键词

VQA，数据增强，Adversarial Learning



## 介绍

近年来，计算机视觉和自然语言处理（NLP）都利用深度学习在许多问题上取得了巨大进展。

视觉问题回答（VQA）是一个融合了计算机视觉和NLP以取得这些成功的研究领域。

VQA算法的目的是预测参考图像的给定问题的正确答案。

最近的基准研究[17]表明，VQA算法的性能取决于训练数据的数量。

现有的算法总是可以从更多的训练数据中受益匪浅。

这表明，没有人工注释的数据扩充是是提高VQA性能的一个重要尝试，就像它在其他深度学习应用上的成功一样。

现有的数据增强方法通过数据扭曲或过度采样来扩大训练数据集的规模[37]。

数据扭曲对数据进行转换并保留其标签。

典型的例子包括几何和颜色转换、随机擦除、对抗性训练和神经风格转移。

过度取样产生合成实例并将其加入训练集。

数据增强已被证明能有效缓解DNNs的过拟合问题[37]。

然而，由于保持<image、question、answer>三联体在语义上的正确性的挑战，VQA中的数据扩充几乎没有被研究。

几何变换和随机擦除图像都不能保留答案。

例如，当问到电脑的位置是什么时，汽车是在垃圾桶的左边还是右边？

结果是相反的答案。随机抹去与问题相关的图像，就会弄错物体的数量。

这样的转换是不可用的。

在文本方面，想出语言转换的通用规则是很有挑战性的。

NLP中的通用数据增强技术还没有被彻底探索。因此，探讨探讨数据增强技术以促进VQA是不难的。

以前的工作基于图像内容[16]和给定答案[25]生成合理的问题，即视觉问题生成（VQG）。

然而，相当一部分生成的问题要么有语法错误，要么措辞怪异。

此外，他们从同一目标数据集中的问题和图像中学习，因此生成的数据与原始数据的分布相同。

由于训练数据和测试数据通常不具有相同的分布，生成的数据不能帮助缓解过度拟合的问题。

在本文中，我们建议将视觉和文本数据的语义等效对抗性例子作为增强数据来生成。

对抗性例子是经过战略性修改的样本，可以成功地欺骗深度模型，使其做出错误的预测。

然而，这种修改是不可察觉的的，保持了数据的语义，同时驱动对抗性例子的基本分布。

对抗性例子远离了原始数据的分布[41]。在我们的方法中。

视觉上的对抗性例子是由一个无目标的基于梯度的攻击者产生的[24]，而文字上的对抗性例子是可能欺骗VQA模型的转述。

VQA模型（预测一个错误的答案），同时保持问题的语义等同。

对抗性例子的存在不仅揭示了ConvNets有限的泛化能力，而且还对这些模型的实际部署造成了安全威胁。



我们对强基线方法Bottom-Up-Attention 和 Top-Down（BUTD）进行对抗性训练。

Top-Down（BUTD）[2]在VQAv2数据集[13]上用干净的例子和即时产生的对抗性例子进行训练。

我们将对抗性训练视为在训练时间内发挥作用的正则器。

实验结果表明我们提出的对抗性训练框架不仅能更好地提高模型，而且

也提高了模型对对抗性攻击的鲁棒性。

虽然目前很少有研究VQA的数据扩充问题的作品[18,35,33,1]，他们只是生成新的问题或图像。

就我们所知，我们的工作是第一个在VQA中同时增强视觉和文本数据的工作。

总而言之，我们的主要贡献有三个方面。

- 我们建议生成视觉和文本的对抗性例子来增强VQA数据集。我们生成的数据保留了语义并探索了学习决策边界，以帮助提高模型的泛化能力。

- 我们提出了一个对抗性训练方案，使VQA模型能够利用对抗性的正则化能力。

- 我们表明，用我们的方法训练的模型在干净的验证集上达到了65.16%的准确率。

  比虚构的训练对应的模型高出1.84%。

  此外，经过对抗性训练的模型在对抗性例子上的准确率明显提高了21.55%。

## 相关工作

### VQA

大量的VQA算法已经被提出，包括空间注意力[2,44,26,6]、组合方法[4,3,14]和双线汇集方案[10,20]。

空间注意力[2]是最广泛使用的方法之一。

自然和合成图像VQA最广泛使用的方法之一。

先前的工作[19,46,29,31]中的很大一部分是建立在bottom-up top-down (BUTD) 的注意力方法[2]之上的。

我们也选择BUTD作为我们的基线VQA模型。我们没有开发更复杂的应答机，而是提出了一种VQA数

据增强技术。

该技术有可能使现有的VQA方法受益，因为数据对于模型来说是燃料。

### 数据增强

与视觉相比，在为分类问题增加文本方面所做的努力很少。

Wei等人[40]对NLP数据增强的文本编辑技术进行了全面的扩展，并在文本分类方面取得了取得了进展。

然而，我们的研究表明，它可能会降低模型在VQA任务中的表现（见第4节）。

其他工作产生释义[45,28]和添加噪音以平滑文本数据[42]。

比较少的是
有较少的作品[18,33,35,1,30]为VQA学习数据增强。

Kafle等人[18]做了一项开创性的工作，他们通过使用语义注释来生成新的问题
在图像上生成新的问题。[33]的工作是为一个源的QA对自动生成包含的问题。
QA对，但它使用视觉基因组[22]中的额外数据来增加多样性
来增加生成问题的多样性。35]的工作提出了一个循环一致的训练方案。
方案，它生成不同的问题重述并训练模型
这样，预测的答案在生成的问题和原始问题之间
保持一致。

方法[1]采用了一种基于GAN的重新合成技术
来自动删除对象，以加强模型的稳健性，防止
语义的视觉变化。请注意，所有这些方法都是以单一模式（纯文本或纯文字）增强数据。
单一模式的数据（纯文本或纯图像），并且严重依赖复杂的模块
以实现轻微的性能提升

### 对抗性攻击和防御。

近年来，研究工作[38,12] 。
在输入图像中加入不可察觉的扰动，称为对抗性例子。
来评估深度神经网络对这种扰动的鲁棒性
攻击。在NLP界，最先进的文本DNN攻击者[5,7,9]
使用与视觉界不同的方法来生成文本的
对抗性例子。我们的工作受到SCPNs[15]和SEA[34]的启发，这些方法
生成句子的转述作为文本对抗性的例子。同时，以前的工作[12]表明，用对抗性例子进行训练可以提高小数据集（如MNIST）的模型泛化，但在大数据集（如MINIST）上会降低性能。
但在大数据集（如ImageNet）上，在完全监督的情况下，性能会下降。
最近值得注意的工作[41]表明，对抗性训练可以提高模型
的性能，即使在ImageNet上有一个精心设计的训练方案。一些方法[36,43]研究了VQA任务中的对抗性攻击。
然而，他们只是对图像进行了攻击，并没有讨论对抗性的例子如何有利于VQA模型。
例子可以使VQA模型受益。总而言之，对抗性例子如何促进VQA的发展
仍然是一个开放的问题。这项工作揭示了如何利用
作为VQA的增强数据。



## 方法

我们现在介绍我们的数据增强方法，以训练一个鲁棒的VQA模型。

![image-20211126143339778](C:/Users/chen/AppData/Roaming/Typora/typora-user-images/image-20211126143339778.png)



如图1所示，给定一个<image, question, answer>三联体，我们首先生成问题的解析并存储起来，然后，即时生成视觉对抗实例，以获得语义等同的额外训练三联体，被用于接下来的对抗训练方案中。

我们在下面详细描述。

## VQA模型

回答关于图像的问题可以被表述为这样一个问题：

根据参数化的概率指数，在给出图像v和问题q的情况下预测答案。

![image-20211126144256236](C:/Users/chen/AppData/Roaming/Typora/typora-user-images/image-20211126144256236.png)



其中θ代表所有要学习的参数的向量，A是所有
答案的集合。

VQA需要同时解决几个任务，包括视觉和文本输入。
文字输入。

这里我们使用Bottom-Up-Attention and Top-Down (BUTD)[2］

作为我们的骨干模型，因为它已经成为VQA的一个黄金基线。

在BUTD，通过微调的Faster R-CNN提取的特定区域的图像特征
[11]被用作视觉输入。

在本文中，v = {
-→v1,
-→v2, ...,
-→vK}

是一个
从K个图像区域提取的视觉特征的集合，问题是
一系列的词q = {q1, q2, ..., qn}。

画面、问题、答案三者之间的关系
具有很强的语义关系，无论是图像还是问题都不容易被转换，以增加训练。

在保留原始内容的情况下，对图像和问题进行转换以增加训练数据。



## 数据扩增

由于存在影响答案的风险，我们避免直接操作原始输入（即图像和问题）。
，

例如裁剪图像或改变单词的顺序。
受对抗性攻击和防御的启发，我们建议产生
对抗性例子作为额外的训练数据。

在本节中，我们将介绍如何在保留原始标签的情况下生成图像和问题的对抗性例子，以及如何使用这些例子进行训练。







视觉对抗性例子的产生。

对抗式攻击起源于计算机视觉界。

一般来说，首要的目标是在输入数据中加入最少的扰动，以造成所需的误分类。

我们采用了一种高效的基于梯度的攻击手段--迭代快速梯度法（IFGSM）[23]来生成视觉对抗性的例子。

在说明IFGSM之前，我们首先介绍FGSM，因为IFGSM是它的一个扩展。

Goodfellow等人[12]提出了FGSM作为一种简单的方法来生成对抗性的例子。

我们可以将其应用于视觉输入，如

![image-20211126144626002](C:/Users/chen/AppData/Roaming/Typora/typora-user-images/image-20211126144626002.png)

其中，vadv是v的对抗性例子，θ是模型参数的集合。

L(θ, v, q, atrue)表示用于训练VQA模型的成本函数，为敌意扰动的大小。攻击将梯度反向传播到
输入的视觉特征来计算∇vL(θ, v, q, atrue)，同时固定网络的参数。

然后，它通过一个小的方向调整输入（即sign(∇vL(θ, v, q, atrue))），使损失最大化。

由此产生的扰动，vadv, 然后被VQA模型错误地分类（例如，该模型预测了图中的Double。
1).FGSM的一个直接扩展是以较小的步长多次应用它。
步长，称为IFGSM。

v
0
adv = v, vN+1
adv = Clipv, 
v
N
adv + α sign(∇vL(θ, vN
adv, q, atrue))	
(3)
其中Clipv,(A)表示对A的元素进行剪裁，Ai,j被剪裁到范围内
[vi,j - , vi,j + ]，α是每次迭代的步长。在本文中，我们将基于梯度的方法总结为
基于梯度的方法称为VAdvGen(v, q)。
单步的对抗性例子生成方法在只计算一个对抗性图像后生成一个候选的
在只计算一个梯度后生成一个候选的对抗性图像。迭代方法适用于
许多梯度更新。它们通常不依赖于任何模型的近似值，并且通常产生更多有害的对抗性图像。
模型，并且通常在运行更多的迭代时产生更多有害的对抗性例子。
更多的迭代。我们的实验结果表明
BUTD vanilla训练的模型在IFGSM产生的视觉对抗性例子上的准确性∈[0.3, 1.3]时的准确率约为17%-30%。

这意味着对抗性例子具有与正常例子的分布不同。



语义等价问题的生成。为了生成问题的对抗性例子qadv，我们不能直接应用图像DNN的方法。
攻击者，因为文本数据是离散的。此外，以图像中的Lp准则衡量的扰动大小也不适用。
用图像中的Lp准则衡量的扰动大小也不适用于文本数据。此外。
文本中的微小变化，如字符或词的变化，将很容易破坏
语法和语义，导致攻击失败的可能性。坚持
在[15,28]的启发下，坚持不改变输入数据的语义的原则。
我们通过使用顺序-顺序的转述模型来生成语义等同的对抗性问题。
这里我们提出了一个纯粹基于神经网络的转述模型
它是基本的编码器-解码器神经机器翻译（NMT）框架的一个扩展。在神经编码器-解码器框架中，编码器
(RNN)被用来将源句的含义压缩成一串
的向量。解码器是一个有条件的RNN语言模型，它生成一个目标句子。
逐字生成目标句子。编码器采用一连串的原始问题词
X = {x1, ..., xTx
}作为输入，并产生一连串的上下文。解码器
产生，给定源句子，目标句子的概率分布
句子Y =

y1, ..., yTy
	
的概率分布，并有一个softmax函数。



然而，在意译的情况下，没有一条从英语到英语的路径。
英语，但可以使用从英语到支点语言再到英语的路径。



例如，源英语句子E1被翻译成一个法语句子F。
接下来，F被翻译回英语，得到一个英语句子的概率分布，E2，作为意译分布。
P(E2|E1, F) = P(E2|F) (5)
我们的意译模型通过K-best译文集F =进行透视。
{F1, ..., Fk}的E1。这确保了源句的多个方面（语义和句法）都能被捕获。
源句子的多个方面（语义和句法）被捕捉到。将多个支点句子翻译成一个
句子产生一个目标词汇的概率分布，可以形成为
形成。
P(yt = w|y<t, F) = X
K
i=1
P(Fi
|E1）- P（yt = w|y<t, Fi） (6)
我们通过对多种语言的多个句子进行透视，进一步扩展多透视方法
多语言的句子（如葡萄牙语）进行透视。从公式6推导出，我们得到P（yt = w|y<t, F
F r）和P（yt = w|y<t, F
P o）。然后对这两个分布进行平均化
分布，产生一个多句子、多语言的转述概率。
P(yt = w|y<t, F
F r
, F
P o) = 1
2
(P(yt = w|y<t, F
F r）+ P（yt = w|y<t, F
P o)) (7)
用来得到句子的概率分布。
P(E2|E1) =
T
YE2
t=1
P(yt|y<t, F
解释
, F
P o) (8)
我们采用预先训练好的NMT模型3，该模型针对英语↔葡萄牙语和英语↔法语进行了训练，以生成候选译文。一个分数[34]，用来
分数[34]，用于衡量译文和原文之间的语义相似度，定义为
定义为。
S(q, q0
) = 最小 
1,
P(q
0
|q)
P(q|q)

(9)
其中P(q
0
|q）是指在原问题q的情况下，解析q
0的概率，原问题q
在公式8中定义的P(q|q)，它近似于恢复q的困难程度。
用来对不同的分布进行标准化。我们对那些编辑距离超过e的候选者或未知词进行惩罚。
编辑距离超过e的候选者或未知词，在相似性分数上加上一个大的负数
λ来惩罚那些编辑距离超过e的候选词或未知词，在相似度分数上加上一个大的负数。我们选择具有最高-k语义分数的转述候选词作为我们的qadv。
语义得分最高的候选词作为我们的qadv。qadv的生成算法表示为qadv =
QAdvGen(q)。
我们的意译至少要编辑单词以保持句法和语义，而不是探索语言的变化。
而不是不顾被感知的可能性去探索语言上的变化。
被感知的可能性。我们在图2中说明了我们的qadv的两个例子。它们表明



产生的转述很容易 "破坏 "BUTD模型。一个预测的标签
如果它与相应的原始问题上的预测不同，则被认为是翻转的。
问题（假设我们在这部分不攻击视觉数据）。我们观察到
qadv不仅从正面预测翻转到负面预测，而且还将负面预测纠正为正面预测。
在某些情况下也会将负面预测修正为正面预测。令人惊讶的是，虚构模型的翻转率
的翻转率为36.72%，绝对准确率下降了10%。
这表明在模型决策中存在脆性，并表明该模型利用了虚假的相关性。
模型在进行预测时利用了虚假的相关关系。



3.3 用增强的例子进行对抗性训练
考虑到对抗性训练框架[24,41]，我们将对抗性例子作为额外的训练样本，用对抗性和干净的例子的混合物训练网络。
训练网络。增强的问题是与模型无关的，并在训练前生成。
并在训练前生成，而可视化的对抗性例子则在训练的每一步持续生成。
训练的每一步都产生。有两种视觉对抗性例子
视觉对抗的例子，取决于问题的输入。
vqc = VAdvGen(v, q), vqadv = VAdvGen(v, qadv) (10)
对于每个（v，q）对，我们有4个额外的训练对，（vqc，q），（vqadv，q）。
(vqc, qadv)和(vqadv, qadv)。所有这四个对在语义上是等同的
这意味着它们拥有相同的基础真理答案。我们保持原来的
语境、问题、答案三者之间的关系，但在只有一个例子的情况下，将原始例子至少增加四次。
次，在只产生一个qadv的情况下。我们制定了一个损失函数
允许控制每批中额外配对的相对权重。
损失 = L(θ, v, q, atrue) + w

L(θ, vqc, q, atrue) + L(θ, vqadv, q, atrue)
+ L(θ, vqc, qadv, atrue) + L(θ, vqadv, qadv, atrue)

(11)
其中，L(θ, v, q, atrue)是对一批有真实答案的v和q例子的损失
atrue，w是一个参数，控制对抗性例子的相对权重



中的损失。我们的主要目标是通过利用对抗性例子的正则化能力来提高网络在干净图像上的性能。
我们的主要目标是通过利用对抗性例子的正则化能力来提高干净图像的网络性能。我们根据经验发现
从头到尾混合使用对抗性和清洁的例子进行训练
从头到尾混合使用对抗性例子进行训练，在干净的样本上不能很好地收敛。因此，我们将它们混合在
一段训练时间，并在其余的历时中用干净的例子进行微调。
这不仅提高了在干净样本上的性能，而且还提高了
模型对对抗性例子的稳健性。我们在算法1中介绍了我们的对抗性
训练方案的算法1。












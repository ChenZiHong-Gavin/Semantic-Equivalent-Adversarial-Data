# 用于视觉问答的语义等效对抗性数据扩展

> 视觉问答（Visual Question Answering,VQA）是一项结合计算机视觉和自然语言处理的学习任务。
>
> 计算机视觉主要是对给定图像进行处理，包括图像识别，图像分类等任务。
>
> 自然语言处理主要是对自然语言， 文本形式的内容进行处理以及理解，包括机器翻译，信息检索，生成文本摘要等任务。
>
> 视觉问答是需要对给定图像和问题进行处理，经过一定的视觉问答技术处理过后生成自然语言答案，是对二者的结合。
>
> 目前VQA比较先进方法包括（1）基于融合的方法：MUTAN和BLOCK；（2）基于注意力的方法：DFAF和MLIN；（3）视觉推理方法：Counting、Dual-MFA和Graph；（4）其他VQA方法：MuRel 等。

## 摘要

由于深度神经网络（DNN）的快速发展，视觉问答（VQA）已经取得了巨大成功。

另一方面，数据增强作为DNN的主要技巧之一，已被广泛用于许多计算机视觉任务。

然而，研究VQA的数据增强问题的工作很少。现有的基于图像的增强方案（如旋转和翻转）都不能直接应用于VQA，因为它的语义结构--需要正确地维护<image、question、answerer>三要素。例如，一个与方向相关的问题-答案（QA）对，如果相关的图像被旋转或翻转，这对问题可能不是真的。

本文中，我们没有直接操作图像和问题，而是使用生成的图像和问题的对抗性例子作为作为增强的数据。

增强后的例子不会改变图像和问题中的视觉属性以及问题的语义。
画面、问题、答案的正确性仍然保持。

然后，我们使用对抗性学习来训练一个经典的VQA模型（BUTD）。

我们发现，我们不仅提高了VQAv2的整体性能，而且与基线模型相比，还能有效地抵御对抗性攻击。
源代码可在https://github.com/zaynmi/seada-vqa找到

## 关键词

VQA，数据增强，Adversarial Learning



## 介绍

近年来，计算机视觉和自然语言处理（NLP）都利用深度学习在许多问题上取得了巨大进展。

视觉问题回答（VQA）是一个融合了计算机视觉和NLP以取得这些成功的研究领域。

VQA算法的目的是预测参考图像的给定问题的正确答案。

最近的基准研究[17]表明，VQA算法的性能取决于训练数据的数量。

现有的算法总是可以从更多的训练数据中受益匪浅。

这表明，没有人工注释的数据扩充是是提高VQA性能的一个重要尝试，就像它在其他深度学习应用上的成功一样。

现有的数据增强方法通过数据扭曲或过度采样来扩大训练数据集的规模[37]。

数据扭曲对数据进行转换并保留其标签。

典型的例子包括几何和颜色转换、随机擦除、对抗性训练和神经风格转移。

过度取样产生合成实例并将其加入训练集。

数据增强已被证明能有效缓解DNNs的过拟合问题[37]。

然而，由于保持<image、question、answer>三联体在语义上的正确性的挑战，VQA中的数据扩充几乎没有被研究。

几何变换和随机擦除图像都不能保留答案。

例如，当问到电脑的位置是什么时，汽车是在垃圾桶的左边还是右边？

结果是相反的答案。随机抹去与问题相关的图像，就会弄错物体的数量。

这样的转换是不可用的。

在文本方面，想出语言转换的通用规则是很有挑战性的。

NLP中的通用数据增强技术还没有被彻底探索。因此，探讨探讨数据增强技术以促进VQA是不难的。

以前的工作基于图像内容[16]和给定答案[25]生成合理的问题，即视觉问题生成（VQG）。

然而，相当一部分生成的问题要么有语法错误，要么措辞怪异。

此外，他们从同一目标数据集中的问题和图像中学习，因此生成的数据与原始数据的分布相同。

由于训练数据和测试数据通常不具有相同的分布，生成的数据不能帮助缓解过度拟合的问题。

在本文中，我们建议将视觉和文本数据的语义等效对抗性例子作为增强数据来生成。

对抗性例子是经过战略性修改的样本，可以成功地欺骗深度模型，使其做出错误的预测。

然而，这种修改是不可察觉的的，保持了数据的语义，同时驱动对抗性例子的基本分布。

对抗性例子远离了原始数据的分布[41]。在我们的方法中。

视觉上的对抗性例子是由一个无目标的基于梯度的攻击者产生的[24]，而文字上的对抗性例子是可能欺骗VQA模型的转述。

VQA模型（预测一个错误的答案），同时保持问题的语义等同。

对抗性例子的存在不仅揭示了ConvNets有限的泛化能力，而且还对这些模型的实际部署造成了安全威胁。



我们对强基线方法Bottom-Up-Attention 和 Top-Down（BUTD）进行对抗性训练。

Top-Down（BUTD）[2]在VQAv2数据集[13]上用干净的例子和即时产生的对抗性例子进行训练。

我们将对抗性训练视为在训练时间内发挥作用的正则器。

实验结果表明我们提出的对抗性训练框架不仅能更好地提高模型，而且

也提高了模型对对抗性攻击的鲁棒性。

虽然目前很少有研究VQA的数据扩充问题的作品[18,35,33,1]，他们只是生成新的问题或图像。

就我们所知，我们的工作是第一个在VQA中同时增强视觉和文本数据的工作。

总而言之，我们的主要贡献有三个方面。

- 我们建议生成视觉和文本的对抗性例子来增强VQA数据集。我们生成的数据保留了语义并探索了学习决策边界，以帮助提高模型的泛化能力。

- 我们提出了一个对抗性训练方案，使VQA模型能够利用对抗性的正则化能力。

- 用我们的方法训练的模型在干净的验证集上达到了65.16%的准确率。比虚构的训练对应的模型高出1.84%。

  此外，经过对抗性训练的模型在对抗性例子上的准确率明显提高了21.55%。

## 相关工作

### VQA

大量的VQA算法已经被提出，包括空间注意力[2,44,26,6]、组合方法[4,3,14]和双线汇集方案[10,20]。

空间注意力[2]是最广泛使用的方法之一。

自然和合成图像VQA最广泛使用的方法之一。

先前的工作[19,46,29,31]中的很大一部分是建立在bottom-up top-down (BUTD) 的注意力方法[2]之上的。

我们也选择BUTD作为我们的基线VQA模型。我们没有开发更复杂的应答机，而是提出了一种VQA数

据增强技术。

该技术有可能使现有的VQA方法受益，因为数据对于模型来说是燃料。

### 数据增强

与视觉增强相比，在分类问题方面为文本增强所做的努力很少。

Wei等人[40]对NLP数据增强的文本编辑技术进行了全面的扩展，并在文本分类方面取得了取得了进展。

然而，我们的研究表明，它可能会降低模型在VQA任务中的表现（见第4节）。

其他工作产生paraphrases[45,28]和添加噪音以平滑文本数据[42]。

[18,33,35,1,30]为VQA进行的数据增强工作要更少。

Kafle等人[18]做了一项开创性的工作，他们通过使用语义注释来生成新的问题在图像上生成新的问题。[33]它的工作是为一个源QA对自动生成包含的问题，但它使用视觉基因组[22]中的额外数据来增加生成问题的多样性。[35]这项工作提出了一个循环一致的训练方案，它生成不同的问题重述并训练模型。这样，预测的答案在生成的问题和原始问题之间保持一致。

方法[1]采用了一种基于GAN的重新合成技术来自动删除对象，以加强模型的稳健性，防止语义的视觉变化。

请注意，所有这些方法都是以单一模式（纯文本或纯文字）增强数据，并且严重依赖复杂的模块以实现轻微的性能提升。

![image-20211127143942547](C:/Users/chen/AppData/Roaming/Typora/typora-user-images/image-20211127143942547.png)

### 对抗性攻击和防御。

近年来，研究人员[38,12] 在输入的图像中加入不可察觉的扰动，称为对抗性例子。来评估深度神经网络对这种扰动的鲁棒性攻击。

在NLP界，最先进的文本DNN攻击者[5,7,9]使用与视觉界不同的方法来生成文本的对抗性例子。

我们的工作受到SCPNs[15]和SEA[34]的启发，这些方法生成句子的转述作为文本对抗性的例子。

同时，以前的工作[12]表明，用对抗性例子进行训练可以提高小数据集（如MNIST）的模型泛化，
但在大数据集（如ImageNet）上，在完全监督的情况下，性能会下降。

最近值得注意的工作[41]表明，对抗性训练可以提高模型的性能，即使是在ImageNet上的一个精心设计的训练方案。

一些方法[36,43]研究了VQA任务中的对抗性攻击。

然而，他们只是对图像进行了攻击，并没有讨论对抗性的例子如何有利于VQA模型。

总而言之，对抗性例子如何促进VQA的发展仍然是一个开放的问题。

这项工作揭示了如何利用作为VQA的增强数据。

## 方法

我们现在介绍我们的数据增强方法，以训练一个鲁棒的VQA模型。

![image-20211126143339778](C:/Users/chen/AppData/Roaming/Typora/typora-user-images/image-20211126143339778.png)



如图1所示，给定一个<image, question, answer>三联体，我们首先生成问题的解析并存储起来，然后，即时生成视觉对抗实例，以获得语义等同的额外训练三联体，被用于接下来的对抗训练方案中。

我们在下面详细描述。

## VQA模型

回答关于图像的问题可以被表述为这样一个问题：

根据参数化的概率指数，在给出图像v和问题q的情况下预测答案。

$$
\hat{a}=\arg \max _{a \in \mathcal{A}} p(a \mid v, q ; \theta)
$$
其中θ代表所有要学习的参数的向量，A是所有答案的集合。

VQA需要同时解决几个任务，包括视觉和文本输入。

这里我们使用Bottom-Up-Attention and Top-Down (BUTD)[2］作为我们的骨干模型，因为它已经成为VQA的一个黄金基线。

在BUTD，通过微调的Faster R-CNN提取的特定区域的图像特征[11]被用作视觉输入。

在本文中，设$$
v=\left\{\overrightarrow{v_{1}}, \overrightarrow{v_{2}}, \ldots, \overrightarrow{v_{K}}\right\}
$$是一个从K个图像区域提取的视觉特征的集合，问题是
一系列的词$q=\{q 1, q 2, \ldots, q n\}$。



画面、问题、答案三者之间的关系具有很强的语义关系，无论是图像还是问题都不容易在保留原始内容的情况下，进行转换以增加训练数据。

## 数据扩增

由于存在影响答案的风险，我们避免直接操作原始输入（即图像和问题），例如裁剪图像或改变单词的顺序。
受对抗性攻击和防御的启发，我们建议产生对抗性例子作为额外的训练数据。

在本节中，我们将介绍如何在保留原始标签的情况下生成图像和问题的对抗性例子，以及如何使用这些例子进行训练。

### 视觉对抗性例子的产生

对抗式攻击起源于计算机视觉界。

一般来说，首要的目标是在输入数据中加入最少的扰动，以造成所需的误分类。

我们采用了一种高效的基于梯度的攻击手段--迭代快速梯度法（IFGSM）[23]来生成视觉对抗性的例子。

在说明IFGSM之前，我们首先介绍FGSM，因为IFGSM是它的一个扩展。

Goodfellow等人[12]提出了FGSM作为一种简单的方法来生成对抗性的例子。

我们可以将其应用于视觉输入，如

$$
v_{a d v}=v+\epsilon \operatorname{sign}\left(\nabla_{v} L\left(\theta, v, q, a_{t r u e}\right)\right)
$$
其中，vadv是v的对抗性例子，θ是模型参数的集合。

$
L\left(\theta, v, q, a_{\text {lrue }}\right)
$表示用于训练VQA模型的成本函数

$\epsilon$为敌意扰动的大小

攻击将梯度反向传播到输入的视觉特征，计算∇vL(θ, v, q, atrue)，同时固定网络的
参数。

然后，它通过一个小的方向调整输入（即sign(∇vL(θ, v, q, atrue))），使损失最大化。

由此产生的扰动，vadv, 之后被VQA模型错误地分类（例如，该模型预测了图1中的Double。

FGSM的一个直接扩展是以较小的步长多次应用它，称为IFGSM。
$$
v_{a d v}^{0}=v, \quad v_{a d v}^{N+1}=\text { Clip }_{v, \epsilon}\left\{v_{a d v}^{N}+\alpha \operatorname{sign}\left(\nabla_{v} L\left(\theta, v_{a d v}^{N}, q, a_{t r u e}\right)\right)\right\}
$$
其中Clipv,(A)表示对A的元素进行剪裁，Ai,j被剪裁到范围内
[vi,j - , vi,j + ]，α是每次迭代的步长。在本文中，我们将基于梯度的方法总结为
基于梯度的方法称为VAdvGen(v, q)。

单步的对抗性例子生成方法在只计算一个对抗性图像后生成一个候选的对抗性图像。

迭代方法适用于许多梯度更新。

它们通常不依赖于任何模型的近似值，并且通常产生更多有害的对抗性图像。

我们的实验结果表明BUTD vanilla训练的模型在IFGSM产生的视觉对抗性例子上的准确性∈[0.3, 1.3]时的准确率约为17%-30%。

这意味着对抗性例子具有与正常例子的分布不同。

### 语义等价问题的生成

为了生成问题的对抗性例子qadv，我们不能直接应用攻击图像DNN的方法，因为文本数据是离散的。

此外，以图像中的Lp准则衡量的扰动大小也不适用。

文本中的微小变化，如字符或词的变化，将很容易破坏语法和语义，导致攻击失败的可能性。

在[15,28]的启发下，坚持不改变输入数据的语义的原则，我们通过使用顺序-顺序的转述模型来生成语义等同的对抗性问题。

这里我们提出了一个纯粹基于神经网络的转述模型

它是基本的编码器-解码器神经机器翻译（NMT）框架的一个扩展。

在神经编码器-解码器框架中，编码器(RNN)被用来将源句的含义压缩成一串的向量。

解码器是一个有条件的RNN语言模型，它逐字生成目标句子。

编码器采用一连串的原始问题词X = {x1, ..., xTx}作为输入，并产生一连串的上下文。

给定源句子，解码器产生目标句子的概率分布 Y ={y1, ..., yTy}

并且有一个柔和函数：
$$
P(Y \mid X)=\prod_{t=1}^{T_{y}} P\left(y_{t} \mid y_{<t}, X\right)
$$
然而，在意译的情况下，没有一条从英语到英语的路径，但可以使用从英语到支点语言再到英语的路径。

例如，源英语句子E1被翻译成一个法语句子F。

接下来，F被翻译回英语，得到一个英语句子的概率分布E2
$$
P\left(E_{2} \mid E_{1}, F\right)=P\left(E_{2} \mid F\right)
$$
我们的意译模型通过K-best译文集F ={F1,F2,F3,……,Fk}进行转换。

这确保了源句的多个方面（语义和句法）都能被捕获。

将多个支点句子翻译成一个句子。

可以形成一个词汇的概率分布
$$
P\left(y_{t}=w \mid y_{<t}, \mathcal{F}\right)=\sum_{i=1}^{K} P\left(\mathcal{F}_{i} \mid E_{1}\right) \cdot P\left(y_{t}=w \mid y_{<t}, \mathcal{F}_{i}\right)
$$
我们通过对多种语言（如葡萄牙语）的多个句子进行转换，进一步扩展转换方法。

从公式6推导出，我们得到$P\left(y_{t}=w \mid y_{<t}, \mathcal{F}^{F r}\right)$和$P\left(y_{t}=w \mid y_{<t}, \mathcal{F}^{P o}\right)$然后对这两个分布进行平均化分布，产生一个多句子、多语言的转述概率。
$$
P\left(y_{t}=w \mid y_{<t}, \mathcal{F}^{F r}, \mathcal{F}^{P o}\right)=\frac{1}{2}\left(P\left(y_{t}=w \mid y_{<t}, \mathcal{F}^{F r}\right)+P\left(y_{t}=w \mid y_{<t}, \mathcal{F}^{P o}\right)\right)
$$
最后得到句子的概率分布：

$P(E 2 \mid E 1)=\prod_{t=1}^{T_{E_{2}}} P\left(y_{t} \mid y_{<t},{\mathcal{F}}^{F r},{\mathcal{F}}^{P o}\right)$

我们采用预先训练好的NMT模型，该模型针对英语↔葡萄牙语和英语↔法语进行了训练，以生成候选译文。

设置了一个分数[34]，用于衡量译文和原文之间的语义相似度，定义为

$$
S\left(q, q^{\prime}\right)=\min \left(1, \frac{P\left(q^{\prime} \mid q\right)}{P(q \mid q)}\right)
$$
其中P(q0|q）是指在原问题q的情况下，解析q0的概率，原问题q在公式8中定义的P(q|q)，它近似于恢复q的困难程度，用来对不同的分布进行标准化。

我们在在相似性分数上加上一个大的负数λ来惩罚那些编辑距离超过e的候选词或未知词。

我们选择具有最高-k语义分数的转述候选词作为我们的qadv。

qadv的生成算法表示为qadv =QAdvGen(q)



我们的意译至少要编辑单词以保持句法和语义，而不是不顾被感知的可能性去探索语言上的变化。

我们在图2中说明了我们的qadv的两个例子。

![image-20211127163243117](C:/Users/chen/AppData/Roaming/Typora/typora-user-images/image-20211127163243117.png)





它们表明生成的转述很容易 "破坏 "BUTD模型。

一个预测的标签如果它与相应的原始问题上的预测不同，则被认为是翻转的（假设我们在这部分不攻击视觉数据）。

我们观察到qadv不仅从正面预测翻转到负面预测，而且还将负面预测纠正为正面预测。

令人惊讶的是，虚构模型的翻转率为36.72%，绝对准确率下降了10%。

这表明在模型决策中存在脆性，并表明该模型利用了虚假的相关性。



### 用增强的例子进行对抗性训练

考虑到对抗性训练框架[24,41]，我们将对抗性例子作为额外的训练样本，用对抗性和干净的例子的混合物训练网络。

增强的问题是与模型无关的，并在训练前生成。

而可视化的对抗性例子则在训练的每一步持续生成。

有两种视觉对抗性例子，取决于问题的输入。
$$
v_{q c}=\operatorname{VAdvGen}(v, q), \quad v_{q a d v}=\operatorname{VAdvGen}\left(v, q_{a d v}\right)
$$
对于每个（v，q）对，我们有4个额外的训练对，（vqc，q），（vqadv，q），(vqc, qadv)和(vqadv, qadv)。

所有这四个对在语义上是等同的，这意味着它们拥有相同的答案。

我们保持原来的语境、问题、答案三者之间的关系，但在只有一个例子的情况下，将原始例子至少增加四次。

在只产生一个qadv的情况下。我们制定了一个损失函数，允许控制每批中额外配对的相对权重。
$$
\begin{aligned}
&\text { Loss }=L\left(\theta, v, q, a_{t r u e}\right)+w\left(L\left(\theta, v_{q c}, q, a_{t r u e}\right)+L\left(\theta, v_{q a d v}, q, a_{t r u e}\right)\right. \\
&\left.\quad+L\left(\theta, v_{q c}, q_{a d v}, a_{t r u e}\right)+L\left(\theta, v_{q a d v}, q_{a d v}, a_{t r u e}\right)\right)
\end{aligned}
$$

其中，L(θ, v, q, atrue)是对一批有真实答案atrue的v和q例子的损失，w是一个参数，控制对抗性例子的相对权重。



我们的主要目标是通过利用对抗性例子的正则化能力来提高网络在干净图像上的性能。
我们根据经验发现从头到尾混合使用对抗性和清洁的例子进行训练，在干净的样本上不能很好地收敛。

因此，我们将它们混合在一段训练时间，并在其余的历时中用干净的例子进行微调。

这不仅提高了在干净样本上的性能，而且还提高了模型对对抗性例子的稳健性。

我们在算法1中介绍了我们的对抗性训练方案的算法1。

![image-20211127163842438](C:/Users/chen/AppData/Roaming/Typora/typora-user-images/image-20211127163842438.png)



## 实验

### 实验准备

**数据集**

我们在VQAv2[13]上进行了实验。该版本在以前的基础上进行了改进，通过减少数据集中的答案偏差来强调视觉理解。

VQAv2包含443K训练，214K验证和453K测试。除远程评估服务器外，测试集的注释是不可用的。

我们提供了验证集和测试集的结果，并在验证集上进行了消融研究。

**VQA架构**

我们使用一个强大的基线Bottom-Up-Attention and TopDown (BUTD) [2]，它结合了自下而上和自上而下的注意力机制来执行VQA，

自下而上的机制从Faster R-CNN[11]中生成对象建议，自上而下的机制预测这些建议的注意力分布。

该模型在2017年VQA挑战赛中获得第一名。

按照[2,39]中的设置，我们在每幅图像中最多使用100个物体建议，即2048维的特征，作为视觉输入。

训练细节。为了进行公平的比较，我们使用Adamax[21]训练BUTD基线和我们的
我们使用Adamax[21]来训练BUTD基线和我们的框架，训练的批次大小为256，共25个epoch。总共25个epochs。基线在验证集上达到了63.32%的准确率，并且 我们保存这个检查点来评估下面的攻击者。为了增加 数据，我们在不同阶段调整学习率。我们设定 初始学习率为0.001，然后在五个 epochs之后以每两个 epochs 0.25的速度递减
每两个历时中衰减0.25。我们仅仅在以下时间段内注入额外的数据
epochs（开始，结束），其中开始是我们开始对抗性训练的epoch
和结束是我们开始标准训练的历时。我们将IFGSM的迭代次数
我们将IFGSM的迭代次数n设为2，每个问题生成的译文数量为1，以节省训练时间。
为1，以节省训练时间。在生成释义时，我们设置编辑距离
阈值e=4，惩罚分数λ=-10。为了避免标签泄露效应
[24]，我们将公式2和3中的真实标签替换为对抗训练时模型预测的最可能的标签。
模型在对抗性训练时最有可能的标签。我们的最佳结果是通过使用
值=0.3，α=0.0625，w=50。这些超参数的选择是基于
网格搜索，并在消融研究中测试了其他设置。



结果

总体性能。表1显示了VQAv2验证集、testdev和test-std集的结果。我们将我们的方法与BUTD虚构的训练
设置进行比较。我们的方法优于香草训练基线，取得了1.82%的收益。
2.55%，2.6%。此外。
我们的对抗性训练方法只消耗了少量的额外的
时间（一个epoch为4分钟），但却能使标准的
准确性。

与其他数据增强方法的比较。我们将我们的
我们的方法与相关的VQA数据增强方法CC[35]和NLP数据
扩增方法[40]，并在表1中报告了VQAv2的结果。
CC的模型被训练来预测一个问题及其改写的相同（正确）答案，这些答案是由其训练方案中的VQG模块生成的。
方案中由VQG模块生成。他们优于验证的准确率与测试-发展集上不太具有竞争力的准确率形成对比。这表明CC在未见过的数据上的概括能力较差。
在未见过的数据上的泛化能力。其他相关研究（如CausalVQA[1], CTM[33]）探讨了
VQA数据增强作为构建新VQA的补充结果
数据集，并在新的数据集上评估他们的数据扩充方法。
而不是VQAv2，所以很难将我们的方法与他们进行比较。EDA是一种文本
编辑技术，可以提高模型在文本分类任务中的性能。我们
实施它，为每个原始问题生成三个增强的问题，并设置问题中的单词百分比。
设置问题中被改变的单词百分比为α=0.1。然而，结果（见表1）显示，EDA可能会降低干净数据的性能
并使准确率下降了0.59%。这表明，文本编辑技术

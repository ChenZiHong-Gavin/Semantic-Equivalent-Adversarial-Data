视频链接：



视频简介：



视频脚本：

1. 导入

首先我们看图片回答几个弱智的问题

图片里的生物是人吗？

他们在干什么？

谁在踢球?

现在在下雨吗？

不用怀疑自己的智商。

作为人类，我们可以很轻松回答这些问题，但是如果把这些问题交给计算机来回答，似乎有些困难。

然而，随着深度学习（DL）的出现，我们目睹了视觉问答的巨大发展

Visual Question Answer (VQA) 

即视觉图像的自然语言问答

方面的巨大研究进展

使得能够回答这些问题的智能系统正在出现



对于深度学习来说，现在的算法总是能够从更多的训练数据中受益匪浅，

也就是说，你投喂给算法越多的数据，它就可能变得越精准。

因此，为了获得更好的数据模型，我们需要对训练的数据进行扩增。

数据扩增，意思是在**不实质性的增加数据**的情况下，让**有限的数据产生等价于更多数据的价值**。

深度学习的训练集都是非常巨大的，因此，这个扩增的工作如果交给人来做，会非常费力，由此我们提出了用于视觉问答的语义等效对抗性数据扩增方法。

我基于论文Semantic Equivalent Adversarial Data Augmentation for Visual Question Answering对工具尝试进行了复现。这个视频是对复现工具运行过程和关键技术的一个展示。



首先我们要实现一个VQA框架

VQA的原理很简单，就是给定一个图像和问题，根据图像中提取的特征回答问题。



我决定使用VGG16进行图像处理，斯坦福大学的Word2Vec进行文本转义，合成图像和词嵌入的特征并运行一个多层感知器，实现一个简单的VQA demo。



演示在jupyter notebook上运行

首先进入anaconda虚拟环境，运行notebook

找到我们的文件，确定kernel环境，然后按照代码顺序

依次导入目录，载入模型，我们使用预训练的vgg16模型

通过vgg16获取图像特征，

通过word2vec获取问题特征，

定义VQA模型

然后进行预测，

我们输入图像和问题，

让模型进行回答

可以看到模型的答案相对比较准确。



为了进一步优化模型，我们要对输入内容进行扩增。

对于图像，我们采取IFGSM方法生成对抗性用例。

对抗攻击是对输入样本故意添加一些人无法察觉的细微的干扰，导致模型以高置信度给出一个错误的输出

FGSM是Goodfellow中描述的最简单的生成对抗图像的方法之一。

FGSM通过线性化损失函数并求解扰动来最大化受$L_{\infty}$约束的扰动，可以以解析解的方式完成此操作，但需要进行一次反向传播的调用：

IFGSM则是以较小的步长将FGSM多次应用，并在每一步之后剪切中间结果的像素值，以确保它们位于原始图像的值域中

分为有目标的对抗攻击和无目标的对抗攻击，它们的原理都基本相似，我们不妨先看一下有目标的对抗攻击。



IFGSM方法同样通过notebook实现



加载vgg16预训练的模型。

给它一个桔子照片的输入，看它认不认识

得到的结果在这里，看样子它目前为止是认识的。

然后我们干一件坏事，在这里获取计算损失函数，求解得到一个最小扰动，将这个扰动加入图像，

经过几次迭代，形成一张新的图像。

我们可以看出，肉眼很难发现这两张图片的区别。

将这张照片送入模型，获得预测结果。

发现，模型会误以为这张图片是一个黄瓜。



这是将黄瓜作为目标攻击时，预测黄瓜的概率随迭代次数的变化。

这是无目标攻击的时候，预测是桔子的概率随迭代次数的变化。










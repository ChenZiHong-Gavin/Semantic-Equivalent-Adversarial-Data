{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1a: 词汇嵌入"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当加载我们的csv文件时，我们可以看到我们的问题和答案是以一种奇怪的方式排列。\n",
    "\n",
    "最好的办法是在一栏中写上问题，另一栏中写上它所指的图片，第三栏中写上答案。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what is on the right side of the black telephone and on the left side of the red chair in the image3 ?\n",
      "desk\n",
      "what is in front of the white door on the left side of the desk in the image3 ?\n",
      "telephone\n",
      "what is on the desk in the image3 ?\n",
      "book  scissor  papers  tape_dispenser\n",
      "what is the largest brown objects in this image3 ?\n",
      "carton\n",
      "what color is the chair in front of the white wall in the image3 ?\n",
      "red\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>what is on the right side of the black telepho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>desk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>what is in front of the white door on the left...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>telephone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>what is on the desk in the image3 ?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0  what is on the right side of the black telepho...\n",
       "1                                               desk\n",
       "2  what is in front of the white door on the left...\n",
       "3                                          telephone\n",
       "4                what is on the desk in the image3 ?"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "# 从文件路径中读取数据\n",
    "data = pd.read_csv(\"./data/raw_data/DAQUAR_train_raw.csv\",header=None)\n",
    "# 查看数据的前五行\n",
    "for i in range(10):\n",
    "    print(data[0][i])\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The questions are every even row and the answers are every odd row. So we go through each row, check if even or odd and rewrite correctly in a csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import csv\n",
    "\n",
    "def prepare_data(in_directory,out_directory, mode):\n",
    "    # 找到原来的数据\n",
    "    file_name_in=os.path.join(in_directory,'DAQUAR_{}_raw.csv'.format(str(mode)))\n",
    "    file_name_out=os.path.join(out_directory,'DAQUAR_{}_processed.csv'.format(str(mode)))\n",
    "    \n",
    "    # 打开文件\n",
    "    with open(file_name_in, 'r') as f, open(file_name_out, 'w', newline='') as f_out:\n",
    "        reader = csv.reader(f)\n",
    "        \n",
    "        fieldnames=['question','image','answer']\n",
    "        writer = csv.DictWriter(f_out, fieldnames=fieldnames)\n",
    "        \n",
    "        writer.writeheader()\n",
    "        \n",
    "        # 问题列，答案列\n",
    "        row_skip=2\n",
    "        dico={'question':None,\n",
    "              'image':None,\n",
    "              'answer':None}\n",
    "\n",
    "        for index, row in enumerate(reader):\n",
    "            \n",
    "            #even number = question\n",
    "            if index % row_skip ==0:\n",
    "                #split the question at the 'image' key word\n",
    "                question_image_list=row[0].split('image')\n",
    "\n",
    "                dico['question']=[question_image_list[0]]\n",
    "                \n",
    "                #remove the question-mark and rewrite 'image' -> useful for integrating visual features later\n",
    "                dico['image']='image'+question_image_list[1].replace(' ?','')\n",
    "            \n",
    "            else:\n",
    "                dico['answer']=row\n",
    "                \n",
    "                #write row in the csv\n",
    "                writer.writerow({'question': dico['question'], 'image':dico['image'], 'answer': dico['answer']})\n",
    "\n",
    "                dico={'question':None,\n",
    "                 'image':None,\n",
    "                'answer':None}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/processed_data\\\\DAQUAR_train_processed.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-36afc06a10a5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprepare_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0min_directory\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'./data/raw_data'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_directory\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'./data/processed_data'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'train'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprepare_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0min_directory\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'./data/raw_data'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_directory\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'./data/processed_data'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'test'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-1a26e1dfde2b>\u001b[0m in \u001b[0;36mprepare_data\u001b[1;34m(in_directory, out_directory, mode)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;31m# 打开文件\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_name_in\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_name_out\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf_out\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[0mreader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/processed_data\\\\DAQUAR_train_processed.csv'"
     ]
    }
   ],
   "source": [
    "prepare_data(in_directory='./data/raw_data', out_directory='./data/processed_data', mode='train')\n",
    "prepare_data(in_directory='./data/raw_data', out_directory='./data/processed_data', mode='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>question</td>\n",
       "      <td>image</td>\n",
       "      <td>answer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['﻿what is on the right side of the black tele...</td>\n",
       "      <td>image3</td>\n",
       "      <td>['desk']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['what is in front of the white door on the le...</td>\n",
       "      <td>image3</td>\n",
       "      <td>['telephone']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['what is on the desk in the ']</td>\n",
       "      <td>image3</td>\n",
       "      <td>['book  scissor  papers  tape_dispenser']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['what is the largest brown objects in this ']</td>\n",
       "      <td>image3</td>\n",
       "      <td>['carton']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0       1  \\\n",
       "0                                           question   image   \n",
       "1  ['﻿what is on the right side of the black tele...  image3   \n",
       "2  ['what is in front of the white door on the le...  image3   \n",
       "3                    ['what is on the desk in the ']  image3   \n",
       "4     ['what is the largest brown objects in this ']  image3   \n",
       "\n",
       "                                           2  \n",
       "0                                     answer  \n",
       "1                                   ['desk']  \n",
       "2                              ['telephone']  \n",
       "3  ['book  scissor  papers  tape_dispenser']  \n",
       "4                                 ['carton']  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "data = pd.read_csv(\"./data/processed_data/DAQUAR_train_processed.csv\",header=None)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1b: 创建单词列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "## 读取csv文件\n",
    "\n",
    "train_dir='./data/processed_data/DAQUAR_train_processed.csv'\n",
    "test_dir='./data/processed_data/DAQUAR_test_processed.csv'\n",
    "\n",
    "data_train=pd.read_csv(train_dir)\n",
    "data_test=pd.read_csv(test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\seada\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:458: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "D:\\anaconda\\envs\\seada\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:459: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "D:\\anaconda\\envs\\seada\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:460: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "D:\\anaconda\\envs\\seada\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:461: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "D:\\anaconda\\envs\\seada\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:462: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "D:\\anaconda\\envs\\seada\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:465: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# 确保tensorflow的版本不是2.0\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# 创建Tokenizer实例\n",
    "MAX_WORDS = 3000\n",
    "tokenizer = Tokenizer(num_words = MAX_WORDS, split=' ')\n",
    "\n",
    "tokenizer.fit_on_texts(data_train['question'])\n",
    "tokenizer.fit_on_texts(data_train['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 因为GloVe的词汇量大约有400K，文件的大小非常大。\n",
    "# 因此，我们首先在本地加载GloVe，以提取存在于我们训练词汇中的单词\n",
    "# 并将其保存为一个numpy文件。\n",
    "# 然后再把它上传到Google Colab上。\n",
    "# 由此，我们从一个1GB的文件变成一个3.6MB的文件\n",
    "\n",
    "def create_embedding_matrix(tokenizer,directory,embed_dims):\n",
    "    embeddings_index = {}\n",
    "    with open(directory,encoding='utf8') as f:\n",
    "        #processing the text\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "    \n",
    "    # 得到我们在训练集中的单词列表\n",
    "    word_index=tokenizer.word_index.items()\n",
    "    # 单词嵌入的维度，这里取300\n",
    "    EMBEDDING_DIM=embed_dims\n",
    "    \n",
    "    # 创建一个嵌入矩阵\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "    for word, i in word_index:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            \n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "directory='./data/embedding/glove.6B.300d.txt'\n",
    "embed_dims=300\n",
    "\n",
    "embedding_matrix=create_embedding_matrix(tokenizer, directory,embed_dims)\n",
    "# 把它保存为npy文件\n",
    "np.save('./data/embedding/glove_300d_embedding.npy', embedding_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: 视觉特征\n",
    "我们从VGG19的最后一层中获得视觉特征。并将这些特征中的每一个附加到正确的问题/答案上，作为一个deque中的元组，这就像一个列表，但更强大。然后，这个deque文件将被加载到我们的Colab笔记本的内存中，以便训练我们的问题-回答模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 重复之前的步骤\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "train_dir='./data/processed_data/DAQUAR_train_processed.csv'\n",
    "test_dir='./data/processed_data/DAQUAR_test_processed.csv'\n",
    "\n",
    "data_train=pd.read_csv(train_dir)\n",
    "data_test=pd.read_csv(test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2.1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "MAX_WORDS = 3000\n",
    "tokenizer = Tokenizer(num_words = MAX_WORDS, split=' ')\n",
    "\n",
    "tokenizer.fit_on_texts(data_train['question'])\n",
    "tokenizer.fit_on_texts(data_train['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences \n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def tokenization(tokenizer, length_of_sequence, dataset, multiple_answer=True):\n",
    "    MAX_LEN=length_of_sequence\n",
    "\n",
    "    seqs_question = tokenizer.texts_to_sequences(dataset['question'])\n",
    "    seqs_answer = tokenizer.texts_to_sequences(dataset['answer'])\n",
    "\n",
    "    #'post'时，如果句子太长，你会把句子末尾的字去掉\n",
    "    pad_seqs_question = pad_sequences(seqs_question,MAX_LEN,truncating='post')\n",
    "    pad_seqs_answer = pad_sequences(seqs_answer,MAX_LEN,truncating='post')\n",
    "\n",
    "    #选择保留一个还是多个回答\n",
    "    if multiple_answer is False:\n",
    "        pad_seqs_answer_one_answer = pad_seqs_answer[:,[MAX_LEN-1]]\n",
    "        return pad_seqs_question, dataset['image'], pad_seqs_answer_one_answer\n",
    "\n",
    "    else:\n",
    "        return pad_seqs_question, dataset['image'], pad_seqs_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#问题的最大词数\n",
    "MAX_LEN=25\n",
    "\n",
    "train_questions,train_images,train_answers = tokenization(tokenizer, MAX_LEN, data_train, multiple_answer=False)\n",
    "test_questions,test_images,test_answers = tokenization(tokenizer, MAX_LEN, data_test, multiple_answer=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这是一个json文件，有我们需要的每张图片的所有视觉特征。它是一个字典，你可以通过使用'imageX'来调用一个图像\n",
    "\n",
    "\n",
    "ex: feat['image3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/img_features.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-0f742bf3109c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#load the visual features into memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./data/img_features.json'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mfeat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/img_features.json'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# 加载视觉特征\n",
    "with open('./data/img_features.json', 'r') as f:\n",
    "    feat = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "#使用try-catch结构防止图片的文件名写错了\n",
    "def fill_deque_with_data(visual_features,questions,images,answers,a_deque):\n",
    "    \n",
    "    error=0\n",
    "    index_error_images=[]\n",
    "\n",
    "    for i in range(len(questions)):\n",
    "        image_name=images[i]\n",
    "        try:\n",
    "            a_deque.append((questions[i],visual_features[image_name],answers[i]))\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            error+=1\n",
    "            index_error_images.append(i)\n",
    "\n",
    "    return error, index_error_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'image10 behind the door frame in fornt of the cabinet in the '\n",
      "'image912 close to the wall in the '\n",
      "'image116 close to the shelf in the '\n",
      "'image135 that is on the counter in the '\n",
      "'image139 on the counter in the '\n",
      "'image95 behind the clothes in the '\n",
      "'image114 on the table in the '\n",
      "'image929 in the '\n",
      "'image1007 in the '\n",
      "'image1008 in the '\n",
      "'image1008 in the '\n",
      "'image1035 in the '\n",
      "'image1043 in the '\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "### TRAINING SET ####\n",
    "train_deque=deque()\n",
    "error, index = fill_deque_with_data(visual_features=feat,\n",
    "                                    questions=train_questions,\n",
    "                                    images=train_images,\n",
    "                                    answers=train_answers,\n",
    "                                    a_deque=train_deque)\n",
    "\n",
    "#保存为txt文件\n",
    "pickleFile = open(\"./data/processed_data/questions-visual_features-train.txt\", 'wb')\n",
    "pickle.dump(train_deque, pickleFile)\n",
    "pickleFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'image1206 on the floor in the '\n",
      "'image1285 on the floor in the '\n",
      "'image1170 which contains some book in the '\n",
      "'image1400 in the mirror reflection in the '\n",
      "'image155 made of in the '\n",
      "'image168 in the '\n",
      "'image1011 in the '\n",
      "'image1407 in the '\n"
     ]
    }
   ],
   "source": [
    "### TEST SET ####\n",
    "test_deque=deque()\n",
    "error, index = fill_deque_with_data(visual_features=feat,\n",
    "                                    questions=test_questions,\n",
    "                                    images=test_images,\n",
    "                                    answers=test_answers,\n",
    "                                    a_deque=test_deque)\n",
    "\n",
    "#save as a text file \n",
    "pickleFile = open(\"./data/processed_data/questions-visual_features-test.txt\", 'wb')\n",
    "pickle.dump(test_deque, pickleFile)\n",
    "pickleFile.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stm32h7",
   "language": "python",
   "name": "stm32"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
